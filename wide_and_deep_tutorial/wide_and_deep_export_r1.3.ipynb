{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep Tutorial\n",
    "**tfversion**: v1.3.0-rc2-20-g0787eee\n",
    "**commit**: 41881b93b1a2b766b69602eb79d3a0514043b7e3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "    \"income_bracket\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical base columns.\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"gender\", [\"Female\", \"Male\"])\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"education\", [\n",
    "        \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n",
    "        \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n",
    "        \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n",
    "        \"Preschool\", \"12th\"\n",
    "    ])\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"marital_status\", [\n",
    "        \"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n",
    "        \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\"\n",
    "    ])\n",
    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"relationship\", [\n",
    "        \"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n",
    "        \"Other-relative\"\n",
    "    ])\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"workclass\", [\n",
    "        \"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n",
    "        \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\"\n",
    "    ])\n",
    "\n",
    "# To show an example of hashing:\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    \"occupation\", hash_bucket_size=1000)\n",
    "native_country = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    \"native_country\", hash_bucket_size=1000)\n",
    "\n",
    "# Continuous base columns.\n",
    "age = tf.feature_column.numeric_column(\"age\")\n",
    "education_num = tf.feature_column.numeric_column(\"education_num\")\n",
    "capital_gain = tf.feature_column.numeric_column(\"capital_gain\")\n",
    "capital_loss = tf.feature_column.numeric_column(\"capital_loss\")\n",
    "hours_per_week = tf.feature_column.numeric_column(\"hours_per_week\")\n",
    "\n",
    "# Transformations.\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wide columns and deep columns.\n",
    "base_columns = [\n",
    "    gender, education, marital_status, relationship, workclass, occupation,\n",
    "    native_country, age_buckets,\n",
    "]\n",
    "\n",
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(\n",
    "        [\"education\", \"occupation\"], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [age_buckets, \"education\", \"occupation\"], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [\"native_country\", \"occupation\"], hash_bucket_size=1000)\n",
    "]\n",
    "\n",
    "deep_columns = [\n",
    "    tf.feature_column.indicator_column(workclass),\n",
    "    tf.feature_column.indicator_column(education),\n",
    "    tf.feature_column.indicator_column(gender),\n",
    "    tf.feature_column.indicator_column(relationship),\n",
    "    # To show an example of embedding\n",
    "    tf.feature_column.embedding_column(native_country, dimension=8),\n",
    "    tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "    age,\n",
    "    education_num,\n",
    "    capital_gain,\n",
    "    capital_loss,\n",
    "    hours_per_week,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(train_data, test_data):\n",
    "    \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "    if train_data:\n",
    "        train_file_name = train_data\n",
    "    else:\n",
    "        train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "            train_file.name)  # pylint: disable=line-too-long\n",
    "        train_file_name = train_file.name\n",
    "        train_file.close()\n",
    "        print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "    \n",
    "    if test_data:\n",
    "        test_file_name = test_data\n",
    "    else:\n",
    "        test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "            test_file.name)  # pylint: disable=line-too-long\n",
    "        test_file_name = test_file.name\n",
    "        test_file.close()\n",
    "        print(\"Test data is downloaded to %s\"% test_file_name)\n",
    "    \n",
    "    return train_file_name, test_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "    \"\"\"Build an estimator.\"\"\"\n",
    "    if model_type == \"wide\":\n",
    "        m = tf.estimator.LinearClassifier(\n",
    "            model_dir=model_dir, feature_columns=base_columns + crossed_columns)\n",
    "    elif model_type == \"deep\":\n",
    "        m = tf.estimator.DNNClassifier(\n",
    "            model_dir=model_dir,\n",
    "            feature_columns=deep_columns,\n",
    "            hidden_units=[100, 50])\n",
    "    else:\n",
    "        m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "            model_dir=model_dir,\n",
    "            linear_feature_columns=crossed_columns,\n",
    "            dnn_feature_columns=deep_columns,\n",
    "            dnn_hidden_units=[100, 50])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle):\n",
    "    \"\"\"Input builder function.\"\"\"\n",
    "    df_data = pd.read_csv(\n",
    "        tf.gfile.Open(data_file),\n",
    "        names=CSV_COLUMNS,\n",
    "        skipinitialspace=True,\n",
    "        engine=\"python\",\n",
    "        skiprows=1)\n",
    "    # remove NaN elements\n",
    "    df_data = df_data.dropna(how=\"any\", axis=0)\n",
    "    labels = df_data[\"income_bracket\"].apply(lambda x: \">50K\" in x).astype(int)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df_data,\n",
    "        y=labels,\n",
    "        batch_size=100,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        num_threads=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "    \"\"\"Train and evaluate the model.\"\"\"\n",
    "    train_file_name, test_file_name = maybe_download(train_data, test_data)\n",
    "    # Specify file path below if want to find the output easily\n",
    "    model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "  \n",
    "    m = build_estimator(model_dir, model_type)\n",
    "    # set num_epochs to None to get infinite stream of data.\n",
    "    m.train(\n",
    "        input_fn=input_fn(train_file_name, num_epochs=None, shuffle=True),\n",
    "        steps=train_steps)\n",
    "    # set steps to None to run evaluation until all data consumed.\n",
    "    results = m.evaluate(\n",
    "        input_fn=input_fn(test_file_name, num_epochs=1, shuffle=False),\n",
    "        steps=None)\n",
    "    print(\"model directory = %s\" % model_dir)\n",
    "    for key in sorted(results):\n",
    "        print(\"%s: %s\" % (key, results[key]))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is downloaded to /var/folders/qv/glzl2pyj2g15vz3tn5s52c9w0000gn/T/tmpcdmdfssx\n",
      "Test data is downloaded to /var/folders/qv/glzl2pyj2g15vz3tn5s52c9w0000gn/T/tmpc7mwxlnb\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_max': 5, '_session_config': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_model_dir': './tmp', '_save_checkpoints_secs': 600}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt-200\n",
      "INFO:tensorflow:Saving checkpoints for 201 into ./tmp/model.ckpt.\n",
      "INFO:tensorflow:loss = 42.5265, step = 201\n",
      "INFO:tensorflow:global_step/sec: 118.392\n",
      "INFO:tensorflow:loss = 53.4587, step = 301 (0.846 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into ./tmp/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 54.8437.\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-07-01:49:45\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt-400\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-07-01:49:54\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.804128, accuracy_baseline = 0.763774, auc = 0.766977, auc_precision_recall = 0.609426, average_loss = 0.484615, global_step = 400, label/mean = 0.236226, loss = 48.405, prediction/mean = 0.244128\n",
      "model directory = ./tmp\n",
      "accuracy: 0.804128\n",
      "accuracy_baseline: 0.763774\n",
      "auc: 0.766977\n",
      "auc_precision_recall: 0.609426\n",
      "average_loss: 0.484615\n",
      "global_step: 400\n",
      "label/mean: 0.236226\n",
      "loss: 48.405\n",
      "prediction/mean: 0.244128\n"
     ]
    }
   ],
   "source": [
    "m=train_and_eval(model_type='wide_n_deep',\n",
    "                 train_steps=200,\n",
    "                 train_data='',\n",
    "                 test_data='',\n",
    "                 model_dir='./tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export TensorFlow Model using .export_savedmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt-400\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'./serving_savemodel/1504748998/saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./serving_savemodel/1504748998'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I step\n",
    "feature_columns = crossed_columns + deep_columns\n",
    "\n",
    "# II step\n",
    "feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n",
    "\n",
    "# III step\n",
    "export_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n",
    "\n",
    "# IV step\n",
    "servable_model_dir = \"./serving_savemodel\"\n",
    "servable_model_path = m.export_savedmodel(servable_model_dir, export_input_fn)\n",
    "servable_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
